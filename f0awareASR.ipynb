{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh25UT953/1LU+fhbMvd5C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sine2pi/asr_model/blob/main/f0awareASR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aURA2B5rjoa6"
      },
      "outputs": [],
      "source": [
        "!pip install pyworld\n",
        "!pip install torch torchaudio\n",
        "!pip install einops\n",
        "!pip install transformers datasets evaluate\n",
        "!pip install tokenizers\n",
        "\n",
        "import pyworld as pw\n",
        "import os\n",
        "import math\n",
        "import warnings\n",
        "import logging\n",
        "import gzip\n",
        "import base64\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from torch import nn, Tensor\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional, Dict, Union, List, Tuple, Any\n",
        "from functools import partial\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset, Audio\n",
        "from transformers.trainer_seq2seq import Seq2SeqTrainer\n",
        "from transformers.training_args_seq2seq import Seq2SeqTrainingArguments\n",
        "import transformers\n",
        "import evaluate\n",
        "from dataclasses import dataclass\n",
        "\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float32\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "extractor = None\n",
        "tokenizer = None\n",
        "optimizer = None\n",
        "scheduler = None\n",
        "model = None\n",
        "Residual = None\n",
        "MultiheadA = None\n",
        "\n",
        "@dataclass\n",
        "class Dimensions:\n",
        "    vocab: int\n",
        "    text_ctx: int\n",
        "    text_dims: int\n",
        "    text_head: int\n",
        "    text_idx: int\n",
        "    mels: int\n",
        "    aud_ctx: int\n",
        "    aud_dims: int\n",
        "    aud_head: int\n",
        "    aud_idx: int\n",
        "    act: str\n",
        "    debug: List[str]\n",
        "    cross_attn: bool\n",
        "    features: List[str]\n",
        "\n",
        "def plot_waveform(x=None, w=None, p=None, per=None, sample_idx=0, sr=16000, hop_length=160,\n",
        "                                 title=\"\", markers=None, marker_labels=None,\n",
        "                                 show_voiced_regions=True, show_energy=False):\n",
        "    num_plots = sum([x is not None, w is not None, p is not None, per is not None])\n",
        "    if num_plots == 0:\n",
        "        raise ValueError(\"No data to plot. Please provide at least one input tensor.\")\n",
        "    time_spans = []\n",
        "\n",
        "    if w is not None:\n",
        "        w_np = w[sample_idx].detach().cpu().numpy()\n",
        "        if w_np.ndim > 1:\n",
        "            w_np = w_np.squeeze()\n",
        "        time_spans.append(len(w_np) / sr)\n",
        "    if x is not None:\n",
        "        x_np = x[sample_idx].detach().cpu().numpy()\n",
        "        if x_np.shape[0] < x_np.shape[1]:\n",
        "            x_np = x_np.T\n",
        "        time_spans.append(x_np.shape[0] * hop_length / sr)\n",
        "    if p is not None:\n",
        "        p_np = p[sample_idx].detach().cpu().numpy()\n",
        "        if p_np.ndim > 1:\n",
        "            p_np = p_np.squeeze()\n",
        "        time_spans.append(len(p_np) * hop_length / sr)\n",
        "    if per is not None:\n",
        "        per_np = per[sample_idx].detach().cpu().numpy()\n",
        "        if per_np.ndim > 1:\n",
        "            per_np = per_np.squeeze()\n",
        "        time_spans.append(len(per_np) * hop_length / sr)\n",
        "    max_time = max(time_spans) if time_spans else 0\n",
        "    fig, axs = plt.subplots(num_plots, 1, figsize=(14, 4*num_plots), sharex=True)\n",
        "    if num_plots == 1:\n",
        "        axs = [axs]\n",
        "    if show_voiced_regions and per is not None:\n",
        "        per_np = per[sample_idx].detach().cpu().numpy()\n",
        "        if per_np.ndim > 1:\n",
        "            per_np = per_np.squeeze()\n",
        "        t_per = np.arange(len(per_np)) * hop_length / sr\n",
        "        threshold = 0.5\n",
        "        for ax in axs:\n",
        "            for i in range(len(per_np)-1):\n",
        "                if per_np[i] > threshold:\n",
        "                    ax.axvspan(t_per[i], t_per[i+1], color='lightblue', alpha=0.2, zorder=0)\n",
        "    current_ax = 0\n",
        "    if w is not None:\n",
        "        w_np = w[sample_idx].detach().cpu().numpy()\n",
        "        if w_np.ndim > 1:\n",
        "            w_np = w_np.squeeze()\n",
        "        t = np.arange(len(w_np)) / sr\n",
        "        axs[current_ax].plot(t, w_np, color=\"tab:blue\")\n",
        "\n",
        "        if show_energy:\n",
        "            frame_length = hop_length\n",
        "            hop_length_energy = hop_length // 2\n",
        "            energy = []\n",
        "            for i in range(0, len(w_np)-frame_length, hop_length_energy):\n",
        "                frame = w_np[i:i+frame_length]\n",
        "                energy.append(np.sqrt(np.mean(frame**2)))\n",
        "            energy = np.array(energy)\n",
        "            energy = energy / np.max(energy) * 0.8 * max(abs(w_np.min()), abs(w_np.max()))\n",
        "            t_energy = np.arange(len(energy)) * hop_length_energy / sr\n",
        "            axs[current_ax].plot(t_energy, energy, color=\"red\", alpha=0.7, label=\"Energy\")\n",
        "            axs[current_ax].legend(loc='upper right')\n",
        "        axs[current_ax].set_title(\"Waveform\")\n",
        "        axs[current_ax].set_ylabel(\"Amplitude\")\n",
        "        axs[current_ax].set_xlim([0, max_time])\n",
        "        axs[current_ax].grid(True, axis='x', linestyle='--', alpha=0.3)\n",
        "        current_ax += 1\n",
        "\n",
        "    if x is not None:\n",
        "        x_np = x[sample_idx].detach().cpu().numpy()\n",
        "        if x_np.shape[0] < x_np.shape[1]:\n",
        "            x_np = x_np.T\n",
        "        im = axs[current_ax].imshow(x_np.T, aspect=\"auto\", origin=\"lower\", cmap=\"magma\",\n",
        "                                   extent=[0, x_np.shape[0]*hop_length/sr, 0, x_np.shape[1]])\n",
        "        axs[current_ax].set_title(\"Spectrogram\")\n",
        "        axs[current_ax].set_ylabel(\"Mel Bin\")\n",
        "        axs[current_ax].set_xlim([0, max_time])\n",
        "        axs[current_ax].grid(True, axis='x', linestyle='--', alpha=0.3)\n",
        "        current_ax += 1\n",
        "\n",
        "    if p is not None:\n",
        "        p_np = p[sample_idx].detach().cpu().numpy()\n",
        "        if p_np.ndim > 1:\n",
        "            p_np = p_np.squeeze()\n",
        "        t_p = np.arange(len(p_np)) * hop_length / sr\n",
        "        axs[current_ax].plot(t_p, p_np, color=\"tab:green\")\n",
        "        axs[current_ax].set_title(\"Pitch\")\n",
        "        axs[current_ax].set_ylabel(\"Frequency (Hz)\")\n",
        "        axs[current_ax].set_xlim([0, max_time])\n",
        "        axs[current_ax].grid(True, axis='both', linestyle='--', alpha=0.3)\n",
        "        axs[current_ax].set_ylim([0, min(1000, p_np.max() * 1.2)])\n",
        "        current_ax += 1\n",
        "\n",
        "    if per is not None:\n",
        "        per_np = per[sample_idx].detach().cpu().numpy()\n",
        "        if per_np.ndim > 1:\n",
        "            per_np = per_np.squeeze()\n",
        "        t_per = np.arange(len(per_np)) * hop_length / sr\n",
        "        axs[current_ax].plot(t_per, per_np, color=\"tab:red\")\n",
        "        axs[current_ax].set_title(\"Period (Voice Activity)\")\n",
        "        axs[current_ax].set_ylabel(\"periodocity\")\n",
        "        axs[current_ax].set_xlim([0, max_time])\n",
        "        axs[current_ax].grid(True, axis='both', linestyle='--', alpha=0.3)\n",
        "        axs[current_ax].set_ylim([-0.05, 1.05])\n",
        "        axs[current_ax].axhline(y=0.5, color='k', linestyle='--', alpha=0.3)\n",
        "\n",
        "    if markers is not None:\n",
        "        for i, t in enumerate(markers):\n",
        "            label = marker_labels[i] if marker_labels and i < len(marker_labels) else None\n",
        "            for ax in axs:\n",
        "                ax.axvline(x=t, color='k', linestyle='-', alpha=0.7, label=label if i == 0 else None)\n",
        "        if marker_labels:\n",
        "            axs[0].legend(loc='upper right', fontsize='small')\n",
        "    axs[-1].set_xlabel(\"Time (s)\")\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def dict_to(d, device, dtype=dtype):\n",
        "    \"\"\"Because PyTorch should have this built-in but doesn't\"\"\"\n",
        "    return {k: v.to(device, dtype) if isinstance(v, torch.Tensor) else v\n",
        "            for k, v in d.items()}\n",
        "\n",
        "def exists(v):\n",
        "    return v is not None\n",
        "\n",
        "def default(v, b):\n",
        "    return v if exists(v) else b\n",
        "\n",
        "class Conv1d(nn.Conv1d):\n",
        "    def _conv_forward(\n",
        "        self, x: Tensor, weight: Tensor, bias) -> Tensor:\n",
        "        return super()._conv_forward(x, weight.to(x.device, x.dtype), None if bias is None else bias.to(x.device, x.dtype))\n",
        "\n",
        "class Conv2d(nn.Conv2d):\n",
        "    def _conv_forward(\n",
        "        self, x: Tensor, weight: Tensor, bias) -> Tensor:\n",
        "        return super()._conv_forward(x, weight.to(x.device, x.dtype), None if bias is None else bias.to(x.device, x.dtype))\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:\n",
        "        super(Linear, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        init.xavier_uniform_(self.linear.weight)\n",
        "        if bias:\n",
        "            init.zeros_(self.linear.bias)\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.linear(x)\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dims: Union[int, Tensor, List, Tuple],\n",
        "                 eps = 1e-8, elementwise_affine = True):\n",
        "        super(RMSNorm, self).__init__()\n",
        "        if isinstance(dims, int):\n",
        "            self.normalized_shape = (dims,)\n",
        "        else:\n",
        "            self.normalized_shape = tuple(dims)\n",
        "        self.eps = eps\n",
        "        self.elementwise_affine = elementwise_affine\n",
        "        if self.elementwise_affine:\n",
        "            self.weight = nn.Parameter(torch.empty(self.normalized_shape))\n",
        "            init.ones_(self.weight)\n",
        "        else:\n",
        "            self.register_parameter(\"weight\", None)\n",
        "    def forward(self, x):\n",
        "        return F.rms_norm(x, self.normalized_shape, self.weight, self.eps)\n",
        "\n",
        "def LayerNorm(x: Tensor, normalized_shape: Union[int, Tensor, List, Tuple],\n",
        "               weight: Optional[Tensor] = None, bias: Optional[Tensor] = None,\n",
        "               eps: float = 1e-5) -> Tensor:\n",
        "    return F.layer_norm(x, normalized_shape, weight, bias, eps)\n",
        "\n",
        "def get_device():\n",
        "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_dtype():\n",
        "    return torch.float32 if torch.cuda.is_available() else torch.float64\n",
        "\n",
        "def tox():\n",
        "    return {\"device\": get_device(), \"dtype\": get_dtype()}\n",
        "\n",
        "def sinusoids(length, num_chan, max=10000):\n",
        "    assert num_chan % 2 == 0\n",
        "    time_x = np.log(max) / (num_chan // 2 - 1)\n",
        "    inv_time = torch.exp(-time_x * torch.arange(num_chan // 2))\n",
        "    s_time = torch.arange(length)[:, np.newaxis] * inv_time[np.newaxis, :]\n",
        "    return torch.cat([torch.sin(s_time), torch.cos(s_time)], dim=1)\n",
        "\n",
        "class rotary(nn.Module):\n",
        "    def __init__(self, dims, head, max_ctx=1500, theta=10000, radii=False, debug: List[str] = [],\n",
        "                 use_pbias=False, spec_shape=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_pbias = use_pbias\n",
        "        self.last_f0_theta = None\n",
        "        self.debug = debug\n",
        "        self._counter = 0\n",
        "        self.dims = dims\n",
        "        self.head = head\n",
        "        self.head_dim = dims // head\n",
        "        self.max_ctx = max_ctx\n",
        "        self.radii = radii\n",
        "        self.learned_adaptation: bool = False\n",
        "        radius = 1\n",
        "        dim = self.head_dim\n",
        "        self.dim = dim\n",
        "\n",
        "        theta = torch.tensor(theta, device=device, dtype=dtype)\n",
        "        self.theta = nn.Parameter(torch.tensor(theta, device=device, dtype=dtype), requires_grad=True)\n",
        "        self.radius = nn.Parameter(torch.ones(radius, device=device, dtype=dtype), requires_grad=True)\n",
        "        inv_freq = (theta / 220.0) * 700 * (torch.pow(10, torch.linspace(0, 2595 * torch.log10(torch.tensor(1 + 8000/700)), dim // 2, device=device, dtype=dtype) / 2595) - 1) / 1000\n",
        "        self.inv_freq = nn.Parameter(torch.tensor(inv_freq, device=device, dtype=dtype), requires_grad=True)\n",
        "\n",
        "    def update_base(self, f0):\n",
        "        f0 = f0.squeeze(0).to(device, dtype)\n",
        "        theta = f0.mean() + 1e-8\n",
        "        inv_freq = (theta / 220.0) * 700 * (torch.pow(10, torch.linspace(0, 2595 * torch.log10(torch.tensor(1 + 8000/700)), self.dim // 2, device=device, dtype=dtype) / 2595) - 1) / 1000\n",
        "        self.inv_freq.data.copy_(inv_freq)\n",
        "        self.theta.data.copy_(theta)\n",
        "\n",
        "    def return_f0(self, f0=None):\n",
        "        if f0 is not None:\n",
        "            self.f0 = f0\n",
        "            return f0.squeeze(0).to(device, dtype)\n",
        "        elif hasattr(self, 'f0') and self.f0 is not None:\n",
        "            return self.f0.squeeze(0).to(device, dtype)\n",
        "        return None\n",
        "\n",
        "    def get_pitch_bias(self, f0):\n",
        "        if f0 is None:\n",
        "            return None\n",
        "        f0_flat = f0.squeeze().float()\n",
        "        f0_norm = (f0_flat - f0_flat.mean()) / (f0_flat.std() + 1e-8)\n",
        "        f0_sim = torch.exp(-torch.cdist(f0_norm.unsqueeze(1),\n",
        "                                    f0_norm.unsqueeze(1)) * self.pitch_scale)\n",
        "        return f0_sim.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    def f0proj(self, f0):\n",
        "        if f0.ndim == 3:\n",
        "            f0 = f0.squeeze(0)\n",
        "        self.f0_proj = nn.Linear(1, self.head_dim // 2, device=device, dtype=dtype)\n",
        "        f0 = f0.to(device, dtype)\n",
        "        f0 = self.f0_proj(f0.unsqueeze(-1))\n",
        "        if f0.ndim == 3:\n",
        "            f0 = f0.squeeze(0)\n",
        "        return f0.to(device=device, dtype=dtype)\n",
        "\n",
        "    def align_f0(self, ctx):\n",
        "        f0 = self.return_f0()\n",
        "        f0 = self.f0proj(f0)\n",
        "        if f0.dim() == 3:\n",
        "            batch, length, dims = f0.shape\n",
        "            if length == ctx:\n",
        "                return f0\n",
        "            frames = length / ctx\n",
        "            idx = torch.arange(ctx, device=f0.device)\n",
        "            idx = (idx * frames).long().clamp(0, length - 1)\n",
        "            return f0[:, idx, :]\n",
        "        if f0.dim() == 1:\n",
        "            length = f0.shape[0]\n",
        "            if length == ctx:\n",
        "                return f0\n",
        "            frames = length / ctx\n",
        "            idx = torch.arange(ctx, device=f0.device)\n",
        "            idx = (idx * frames).long().clamp(0, length - 1)\n",
        "            return f0[idx]\n",
        "        else:\n",
        "            length, dims = f0.shape\n",
        "            if length == ctx:\n",
        "                return f0\n",
        "            frames = length / ctx\n",
        "            idx = torch.arange(ctx, device=f0.device)\n",
        "            idx = (idx * frames).long().clamp(0, length - 1)\n",
        "            return f0[idx, :]\n",
        "\n",
        "    def forward(self, x=None, f0=None, enc=None, layer=None, input_type=\"audio\") -> Tensor:\n",
        "        if isinstance(x, int):\n",
        "            ctx = x\n",
        "        elif isinstance(x, torch.Tensor) and x.ndim == 3:\n",
        "            batch, ctx, dims = x.shape\n",
        "        else:\n",
        "            batch, head, ctx, head_dim = x.shape\n",
        "        t = torch.arange(ctx, device=device, dtype=dtype)\n",
        "        freqs = self.inv_freq\n",
        "        freqs = t[:, None] * freqs[None, :]\n",
        "\n",
        "        if self.radii:\n",
        "            radius = self.align_f0(ctx)\n",
        "            if \"rotary2\" in self.debug and self._counter == 5:\n",
        "                print(f\"{layer} radius: {radius} ctx: {ctx}\")\n",
        "        else:\n",
        "            radius = freqs\n",
        "        freqs = torch.polar(torch.ones_like(radius), freqs)\n",
        "\n",
        "        if \"rotary3\" in self.debug and self._counter == 5:\n",
        "            print(f\"{layer} count {self._counter} f0: {f0.shape if f0 is not None else None} freqs: {freqs.shape}  radius: {radius.shape} ctx: {ctx}\")\n",
        "            print(f\"freqs mean: {freqs.mean():.2f} inv_freq mean: {self.inv_freq.mean():.2f} theta: {self.theta.item():.2f} radius mean: {radius.mean():.2f} radius shape: {radius.shape} ctx: {ctx}\")\n",
        "\n",
        "        if \"rotary_detail\" in self.debug and self._counter == 5:\n",
        "            print(f\"\\n==== Detailed RoPE Analysis ====\")\n",
        "            print(f\"Layer: {layer}, Context Length: {ctx}\")\n",
        "            print(f\"F0 stats: mean={self.theta.item():.2f}\")\n",
        "            print(f\"inv_freq range: [{self.inv_freq.min().item():.4f}, {self.inv_freq.max().item():.4f}]\")\n",
        "\n",
        "            if self.radii:\n",
        "                print(f\"Radius Shape: {radius.shape}, Mean: {radius.mean().item():.4f}\")\n",
        "                print(f\"Radius[0]: {radius[0][:5].cpu().numpy()}\")\n",
        "                print(f\"Radius[mid]: {radius[ctx//2][:5].cpu().numpy()}\")\n",
        "                print(f\"Radius[end]: {radius[-1][:5].cpu().numpy()}\")\n",
        "\n",
        "            print(f\"Final freqs shape: {freqs.shape}\")\n",
        "            print(f\"Freqs[0]: {freqs[0][:5].cpu().detach().numpy()}\")\n",
        "            print(f\"Freqs[mid]: {freqs[ctx//2][:5].cpu().detach().numpy()}\")\n",
        "            print(f\"Freqs[end]: {freqs[-1][:5].cpu().detach().numpy()}\")\n",
        "            print(\"================================\\n\")\n",
        "\n",
        "        self._counter += 1\n",
        "        return freqs.unsqueeze(0)\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_rotary(x, freqs):\n",
        "        x1 = x[..., :freqs.shape[-1]*2]\n",
        "        x2 = x[..., freqs.shape[-1]*2:]\n",
        "        orig_shape = x1.shape\n",
        "        if x1.ndim == 2:\n",
        "            x1 = x1.unsqueeze(0)\n",
        "        x1 = x1.float().reshape(*x1.shape[:-1], -1, 2).contiguous()\n",
        "        x1 = torch.view_as_complex(x1) * freqs\n",
        "        x1 = torch.view_as_real(x1).flatten(-2)\n",
        "        x1 = x1.view(orig_shape)\n",
        "        return torch.cat([x1.type_as(x), x2], dim=-1)\n",
        "\n",
        "class MultiheadA(nn.Module):\n",
        "\n",
        "    rbf = False\n",
        "    def __init__(self, dims: int, head: int, rotary_emb: bool = True,\n",
        "                 zero_val: float = 1e-4, minz: float = 1e-6, maxz: float = 1e-3, debug: List[str] = [], optim_attn=False):\n",
        "        super(MultiheadA, self).__init__()\n",
        "\n",
        "        self.dims = dims\n",
        "        self.head = head\n",
        "        self.head_dim = dims // head\n",
        "        self.debug = debug\n",
        "        self._counter = 0\n",
        "\n",
        "        self.q = Linear(dims, dims).to(device, dtype)\n",
        "        self.k = Linear(dims, dims, bias=False).to(device, dtype)\n",
        "        self.v = Linear(dims, dims).to(device, dtype)\n",
        "        self.o = Linear(dims, dims).to(device, dtype)\n",
        "\n",
        "        self.pad_token = 0\n",
        "        self.rotary_emb = rotary_emb\n",
        "        self.minz = minz\n",
        "        self.maxz = maxz\n",
        "        self.zero_val = zero_val\n",
        "        self.optim_attn = optim_attn\n",
        "        self.fzero = nn.Parameter(torch.tensor(zero_val, device=device, dtype=dtype), requires_grad=False)\n",
        "\n",
        "        if rotary_emb:\n",
        "            self.rope = rotary(\n",
        "                dims=dims,\n",
        "                head=head,\n",
        "                debug=debug,\n",
        "                radii=False,\n",
        "                )\n",
        "        else:\n",
        "            self.rope = None\n",
        "\n",
        "    def enhanced_attention_scores(self, q, k, rbf_sigma=1.0, rbf_ratio=0.0):\n",
        "        scale = (self.dims // self.head) ** -0.25\n",
        "        dot_scores = torch.matmul(q, k.transpose(-1, -2)) * scale\n",
        "        if rbf_ratio <= 0.0:\n",
        "            return dot_scores\n",
        "        q_norm = q.pow(2).sum(dim=-1, keepdim=True)\n",
        "        k_norm = k.pow(2).sum(dim=-1, keepdim=True)\n",
        "        qk = torch.matmul(q, k.transpose(-1, -2))\n",
        "        dist_sq = q_norm + k_norm.transpose(-1, -2) - 2 * qk\n",
        "        rbf_scores = torch.exp(-dist_sq / (2 * rbf_sigma**2))\n",
        "        return (1 - rbf_ratio) * dot_scores + rbf_ratio * rbf_scores\n",
        "\n",
        "    def forward(self, x: Tensor, xa: Tensor = None, mask: Tensor = None, enc = None, layer = None, feature_type=\"audio\") -> tuple:\n",
        "        x = x.to(device, dtype)\n",
        "        if xa is not None:\n",
        "            xa = xa.to(device, dtype)\n",
        "\n",
        "        batch, ctx, dims = x.shape\n",
        "        scale = (self.dims // self.head) ** -0.25\n",
        "\n",
        "        z = default(xa, x).to(device, dtype)\n",
        "        q = self.q(x)\n",
        "        k = self.k(z)\n",
        "        v = self.v(z)\n",
        "        qlen = q.shape[1]\n",
        "        klen = k.shape[1]\n",
        "\n",
        "        if self.rotary_emb:\n",
        "            q = q.view(*q.shape[:2], self.head, -1).permute(0, 2, 1, 3)\n",
        "            k = k.view(*k.shape[:2], self.head, -1).permute(0, 2, 1, 3)\n",
        "            v = v.view(*v.shape[:2], self.head, -1).permute(0, 2, 1, 3)\n",
        "            qlen = q.shape[2]\n",
        "            klen = k.shape[2]\n",
        "\n",
        "            q = self.rope.apply_rotary(q, (self.rope(qlen, enc=enc, layer=layer)))\n",
        "            k = self.rope.apply_rotary(k, (self.rope(klen, enc=enc, layer=layer)))\n",
        "        else:\n",
        "            q = q.view(*q.shape[:2], self.head, -1).permute(0, 2, 1, 3)\n",
        "            k = k.view(*k.shape[:2], self.head, -1).permute(0, 2, 1, 3)\n",
        "            v = v.view(*v.shape[:2], self.head, -1).permute(0, 2, 1, 3)\n",
        "            batch, head, ctx, head_dim = q.shape\n",
        "\n",
        "        if self.rbf:\n",
        "            qk = self.enhanced_attention_scores(q * scale, k * scale, rbf_sigma=1.0, rbf_ratio=0.3)\n",
        "\n",
        "        qk = (q * scale) @ (k * scale).transpose(-1, -2)\n",
        "        if self.rope.use_pbias:\n",
        "            f0 = enc.get(\"f0\", None) if enc is not None else None\n",
        "            pbias = self.rope.use_pbias(f0)\n",
        "            if pbias is not None:\n",
        "                qk = qk + pbias[:,:,:q.shape[2],:q.shape[2]]\n",
        "        token_ids = k[:, :, :, 0]\n",
        "        zscale = torch.ones_like(token_ids)\n",
        "        fzero = torch.clamp(F.softplus(self.fzero), self.minz, self.maxz)\n",
        "        zscale[token_ids.float() == self.pad_token] = fzero\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask[:q.shape[2], :q.shape[2]]\n",
        "            qk = qk + mask.unsqueeze(0).unsqueeze(0) * zscale.unsqueeze(-2).expand(qk.shape)\n",
        "        qk = qk * zscale.unsqueeze(-2)\n",
        "        w = F.softmax(qk, dim=-1).to(q.dtype)\n",
        "        wv = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n",
        "\n",
        "        if \"multihead\" in self.debug and self._counter % 100 == 0:\n",
        "            print(f\"MHA: q={q.shape}, k={k.shape}, v={v.shape} - {qk.shape}, wv shape: {wv.shape}\")\n",
        "        self._counter += 1\n",
        "        return self.o(wv), qk.detach()\n",
        "\n",
        "class t_gate(nn.Module):\n",
        "    def __init__(self, dims, num_types=4):\n",
        "        super().__init__()\n",
        "        self.gate_projections = nn.ModuleList([\n",
        "            nn.Sequential(Linear(dims, 1), nn.Sigmoid())\n",
        "            for _ in range(num_types)])\n",
        "        self.type_classifier = nn.Sequential(\n",
        "            Linear(dims, num_types),\n",
        "            nn.Softmax(dim=-1))\n",
        "    def forward(self, x):\n",
        "        type_probs = self.type_classifier(x)\n",
        "        gates = torch.stack([gate(x) for gate in self.gate_projections], dim=-1)\n",
        "        comb_gate = torch.sum(gates * type_probs.unsqueeze(2), dim=-1)\n",
        "        return comb_gate\n",
        "\n",
        "class m_gate(nn.Module):\n",
        "    def __init__(self, dims, mem_size=64):\n",
        "        super().__init__()\n",
        "        self.m_key = nn.Parameter(torch.randn(mem_size, dims))\n",
        "        self.m_val = nn.Parameter(torch.randn(mem_size, 1))\n",
        "        self.gate_proj = nn.Sequential(Linear(dims, dims//2), nn.SiLU(), Linear(dims//2, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        d_gate = torch.sigmoid(self.gate_proj(x))\n",
        "        attention = torch.matmul(x, self.m_key.transpose(0, 1))\n",
        "        attention = F.softmax(attention / math.sqrt(x.shape[-1]), dim=-1)\n",
        "        m_gate = torch.matmul(attention, self.m_val)\n",
        "        m_gate = torch.sigmoid(m_gate)\n",
        "        return 0.5 * (d_gate + m_gate)\n",
        "\n",
        "class c_gate(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super().__init__()\n",
        "        self.s_gate = nn.Sequential(Linear(dims, 1), nn.Sigmoid())\n",
        "        self.w_gate = nn.Sequential(Linear(dims, 1), nn.Sigmoid())\n",
        "        self.p_gate = nn.Sequential(Linear(dims, 1), nn.Sigmoid())\n",
        "        self.e_gate = nn.Sequential(Linear(dims, 1), nn.Sigmoid())\n",
        "        self.ph_gate = nn.Sequential(Linear(dims, 1), nn.Sigmoid())\n",
        "        self.integ = Linear(dims*5, dims)\n",
        "\n",
        "    def forward(self, x, features):\n",
        "        s_feat = features.get(\"spectrogram\", x)\n",
        "        w_feat = features.get(\"waveform\", x)\n",
        "        p_feat = features.get(\"pitch\", x)\n",
        "        e_feat = features.get(\"envelope\", x)\n",
        "        ph_feat = features.get(\"phase\", x)\n",
        "        s = self.s_gate(x) * s_feat\n",
        "        w = self.w_gate(x) * w_feat\n",
        "        p = self.p_gate(x) * p_feat\n",
        "        e = self.e_gate(x) * e_feat\n",
        "        ph = self.ph_gate(x) * ph_feat\n",
        "        comb = torch.cat([s, w, p, e, ph], dim=-1)\n",
        "        return self.integ(comb)\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    _seen = set()\n",
        "    def __init__(self, ctx, dims, head, act, cross_attn=True, debug: List[str] = [],\n",
        "                 tgate=True, mgate=False, cgate=False, mem_size=512, features=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dims = dims\n",
        "        self.head = head\n",
        "        self.ctx = ctx\n",
        "        self.head_dim = dims // head\n",
        "        self.cross_attn = cross_attn\n",
        "        self.features = features\n",
        "        self.debug = debug\n",
        "        self._counter = 0\n",
        "        self.dropout = 0.01\n",
        "\n",
        "        self.t_gate = tgate\n",
        "        self.m_gate = mgate\n",
        "        self.c_gate = cgate\n",
        "\n",
        "        self.blend = nn.Parameter(torch.tensor(0.5))\n",
        "\n",
        "        act_map = {\"gelu\": nn.GELU(), \"relu\": nn.ReLU(), \"sigmoid\": nn.Sigmoid(),\n",
        "                  \"tanh\": nn.Tanh(), \"swish\": nn.SiLU(), \"tanhshrink\": nn.Tanhshrink(),\n",
        "                  \"softplus\": nn.Softplus(), \"softshrink\": nn.Softshrink(),\n",
        "                  \"leaky_relu\": nn.LeakyReLU(), \"elu\": nn.ELU()}\n",
        "        act_fn = act_map.get(act, nn.GELU())\n",
        "\n",
        "        self.attna = MultiheadA(dims, head, rotary_emb=True, debug=debug)\n",
        "        self.attnb = (MultiheadA(dims, head, rotary_emb=True, debug=debug) if cross_attn else None)\n",
        "\n",
        "        mlp = dims * 4\n",
        "        self.mlp = nn.Sequential(Linear(dims, mlp), act_fn, Linear(mlp, dims))\n",
        "\n",
        "        self.t_gate = t_gate(dims=dims, num_types=4) if t_gate else None\n",
        "        self.m_gate = m_gate(dims=dims, mem_size=mem_size) if m_gate else None\n",
        "        self.c_gate = c_gate(dims=dims) if cgate else None\n",
        "\n",
        "        self.lna = RMSNorm(dims)\n",
        "        self.lnb = RMSNorm(dims) if cross_attn else None\n",
        "        self.lnc = RMSNorm(dims)\n",
        "\n",
        "        if not any([t_gate, m_gate, c_gate]):\n",
        "            self.mlp_gate = nn.Sequential(Linear(dims, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x, xa=None, mask=None, enc=None, layer=None, feature_type=\"audio\") -> Tensor:\n",
        "        x = x.to(device, dtype)\n",
        "        if xa is not None:\n",
        "            xa = xa.to(device, dtype)\n",
        "\n",
        "        bln = self.blend\n",
        "        x = x + self.attna(self.lna(x), xa=None, mask=mask, enc=enc, layer=layer)[0]\n",
        "\n",
        "        if self.attnb and xa is not None:\n",
        "            c = self.attnb(self.lnb(x), xa=xa, mask=None, enc=enc, layer=layer)[0]\n",
        "            b = torch.sigmoid(bln)\n",
        "            x = b * x + (1 - b) * c\n",
        "\n",
        "        normx = self.lnc(x)\n",
        "        mlp_out = self.mlp(normx)\n",
        "\n",
        "        if self.t_gate:\n",
        "            gate = self.t_gate(normx)\n",
        "            x = x + gate * mlp_out\n",
        "\n",
        "        elif self.m_gate:\n",
        "            gate = self.m_gate(normx)\n",
        "            x = x + gate * mlp_out\n",
        "\n",
        "        elif self.c_gate:\n",
        "            gate_output = self.c_gate(normx, self.features)\n",
        "            x = x + gate_output\n",
        "\n",
        "        else:\n",
        "            if hasattr(self, 'mlp_gate'):\n",
        "                mlp_gate = self.mlp_gate(normx)\n",
        "                x = x + mlp_gate * mlp_out\n",
        "            else:\n",
        "                x = x + mlp_out\n",
        "\n",
        "        if \"residual\" in self.debug and self._counter % 100 == 0:\n",
        "            print(f\"Step {self._counter}: Residual block output shape: {x.shape}, xa shape: {xa.shape if xa is not None else None}\")\n",
        "            if self.t_gate:\n",
        "                print(f\"Step {self._counter}: Using t_gate: {self.t_gate}\")\n",
        "            elif self.m_gate:\n",
        "                print(f\"Step {self._counter}: Using m_gate: {self.m_gate}\")\n",
        "            elif self.c_gate:\n",
        "                print(f\"Step {self._counter}: Using c_gate: {self.c_gate}\")\n",
        "            else:\n",
        "                print(f\"Step {self._counter}: Using MLP gate: {self.mlp_gate if hasattr(self, 'mlp_gate') else None}\")\n",
        "        self._counter += 1\n",
        "        return x\n",
        "\n",
        "class FEncoder(nn.Module):\n",
        "    def __init__(self, input_dims, dims, head, layer, kernel_size, act, stride=1, use_rope=False, spec_shape=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head = head\n",
        "        self.head_dim = dims // head\n",
        "        self.dropout = 0.01\n",
        "        self.use_rope = use_rope\n",
        "        self.dims = dims\n",
        "\n",
        "        act_map = {\"gelu\": nn.GELU(), \"relu\": nn.ReLU(), \"sigmoid\": nn.Sigmoid(), \"tanh\": nn.Tanh(), \"swish\": nn.SiLU(), \"tanhshrink\": nn.Tanhshrink(), \"softplus\": nn.Softplus(), \"softshrink\": nn.Softshrink(), \"leaky_relu\": nn.LeakyReLU(), \"elu\": nn.ELU()}\n",
        "        act_fn = act_map.get(act, nn.GELU())\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            Conv1d(input_dims, dims, kernel_size=kernel_size, stride=stride, padding=kernel_size//2), act_fn,\n",
        "            Conv1d(dims, dims, kernel_size=5, padding=2), act_fn,\n",
        "            Conv1d(dims, dims, kernel_size=3, padding=1, groups=dims), act_fn)\n",
        "\n",
        "        if use_rope:\n",
        "            if spec_shape is not None:\n",
        "                self.rope = rotary(\n",
        "                    dims=self.head_dim,\n",
        "                    use_2d_axial=True,\n",
        "                    spec_shape=spec_shape, debug=[])\n",
        "            else:\n",
        "                self.rope = rotary(\n",
        "                    dims=self.head_dim,\n",
        "                    use_2d_axial=False, debug=[])\n",
        "        else:\n",
        "            self.rope = None\n",
        "            self.positional = lambda length: sinusoids(length, dims)\n",
        "\n",
        "        self.norm = RMSNorm(dims)\n",
        "        self._norm = RMSNorm(dims)\n",
        "\n",
        "    def apply_rope_to_features(self, x, layer=None, feature_type=\"audio\"):\n",
        "        if feature_type in [\"envelope\", \"phase\"]:\n",
        "            feature_type = \"spectrogram\"\n",
        "        batch, ctx, dims = x.shape\n",
        "        x = x.view(batch, ctx, self.head, self.head_dim).permute(0, 2, 1, 3)\n",
        "        if feature_type == \"spectrogram\" and hasattr(self.rope, 'use_2d_axial') and self.rope.use_2d_axial:\n",
        "            rope_freqs = self.rope(ctx, layer=layer, input_type=\"spectrogram\")\n",
        "        else:\n",
        "            rope_freqs = self.rope(ctx, layer=layer, input_type=\"audio\")\n",
        "        x = self.rope.apply_rotary(x, rope_freqs)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch, ctx, dims)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, enc=None, layer=None, feature_type=\"audio\"):\n",
        "        x = self.encoder(x).permute(0, 2, 1)\n",
        "        if self.use_rope:\n",
        "            x = self.apply_rope_to_features(x, layer=layer, feature_type=feature_type)\n",
        "        else:\n",
        "            x = x + self.positional(x.shape[1]).to(x.device, x.dtype)\n",
        "        x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self._norm(x)\n",
        "        return x\n",
        "\n",
        "class WEncoder(nn.Module):\n",
        "    def __init__(self, input_dims, dims, head, layer, kernel_size, act, use_rope=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head = head\n",
        "        self.head_dim = dims // head\n",
        "        self.dropout = 0.01\n",
        "        self.use_rope = use_rope\n",
        "        self.dims = dims\n",
        "\n",
        "        act_map = {\"gelu\": nn.GELU(), \"relu\": nn.ReLU(), \"sigmoid\": nn.Sigmoid(), \"tanh\": nn.Tanh(), \"swish\": nn.SiLU(), \"tanhshrink\": nn.Tanhshrink(), \"softplus\": nn.Softplus(), \"softshrink\": nn.Softshrink(), \"leaky_relu\": nn.LeakyReLU(), \"elu\": nn.ELU()}\n",
        "        act_fn = act_map.get(act, nn.GELU())\n",
        "\n",
        "        self.downsample = nn.Sequential(\n",
        "            Conv1d(input_dims, dims//8, kernel_size=15, stride=8, padding=7), act_fn,\n",
        "            Conv1d(dims//8, dims//4, kernel_size=7, stride=4, padding=3), act_fn,\n",
        "            Conv1d(dims//4, dims, kernel_size=9, stride=5, padding=4), act_fn)\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            Conv1d(dims, dims, kernel_size=3, padding=1, groups=dims//8),  act_fn,\n",
        "            Conv1d(dims, dims, kernel_size=1), act_fn)\n",
        "        if use_rope:\n",
        "            self.rope = rotary(\n",
        "                dims=self.head_dim,\n",
        "                use_2d_axial=False,\n",
        "                theta=50.0, debug=[])\n",
        "        else:\n",
        "            self.rope = None\n",
        "            self.positional = lambda length: sinusoids(length, dims)\n",
        "        self.norm = RMSNorm(dims)\n",
        "\n",
        "    def apply_rope_to_features(self, x, layer=None):\n",
        "        if not self.use_rope or self.rope is None:\n",
        "            return x\n",
        "        batch, ctx, dims = x.shape\n",
        "        x = x.view(batch, ctx, self.head, self.head_dim).permute(0, 2, 1, 3)\n",
        "        rope_freqs = self.rope(ctx, layer=layer, input_type=\"waveform\")\n",
        "        x = self.rope.apply_rotary(x, rope_freqs)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch, ctx, dims)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, enc=None, layer=None, feature_type=\"waveform\"):\n",
        "        x = self.downsample(x)\n",
        "        x = self.encoder(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        if self.use_rope:\n",
        "            x = self.apply_rope_to_features(x, layer=layer)\n",
        "        else:\n",
        "            x = x + self.positional(x.shape[1]).to(x.device, x.dtype)\n",
        "        x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.norm(x)\n",
        "\n",
        "class PEncoder(nn.Module):\n",
        "    def __init__(self, input_dims, dims, head, layer, kernel_size, act, use_rope=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head = head\n",
        "        self.head_dim = dims // head\n",
        "        self.dropout = 0.01\n",
        "        self.use_rope = use_rope\n",
        "        self.dims = dims\n",
        "\n",
        "        act_map = {\"gelu\": nn.GELU(), \"relu\": nn.ReLU(), \"sigmoid\": nn.Sigmoid(), \"tanh\": nn.Tanh(), \"swish\": nn.SiLU(), \"tanhshrink\": nn.Tanhshrink(), \"softplus\": nn.Softplus(), \"softshrink\": nn.Softshrink(), \"leaky_relu\": nn.LeakyReLU(), \"elu\": nn.ELU()}\n",
        "        act_fn = act_map.get(act, nn.GELU())\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            Conv1d(input_dims, dims//4, kernel_size=7, stride=8, padding=3), act_fn,\n",
        "            Conv1d(dims//4, dims//2, kernel_size=5, stride=4, padding=2), act_fn,\n",
        "            Conv1d(dims//2, dims, kernel_size=5, stride=5, padding=2), act_fn)\n",
        "\n",
        "        if use_rope:\n",
        "            self.rope = rotary(\n",
        "                dims=self.head_dim,\n",
        "                use_2d_axial=False,\n",
        "                theta=100.0, debug=[])\n",
        "        else:\n",
        "            self.rope = None\n",
        "            self.positional = lambda length: sinusoids(length, dims)\n",
        "        self.norm = RMSNorm(dims)\n",
        "\n",
        "    def apply_rope_to_features(self, x, layer=None):\n",
        "        if not self.use_rope or self.rope is None:\n",
        "            return x\n",
        "        batch, ctx, dims = x.shape\n",
        "        x = x.view(batch, ctx, self.head, self.head_dim).permute(0, 2, 1, 3)\n",
        "        rope_freqs = self.rope(ctx, layer=layer, input_type=\"pitch\")\n",
        "        x = self.rope.apply_rotary(x, rope_freqs)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch, ctx, dims)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, enc=None, layer=None, feature_type=\"pitch\"):\n",
        "        x = self.encoder(x).permute(0, 2, 1)\n",
        "        if self.use_rope:\n",
        "            x = self.apply_rope_to_features(x, layer=layer)\n",
        "        else:\n",
        "            x = x + self.positional(x.shape[1]).to(x.device, x.dtype)\n",
        "        x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class AudioEncoder(nn.Module):\n",
        "    _seen = set()\n",
        "    def __init__(self, mels: int, ctx: int, dims: int, head: int, layer: int, debug: List[str], features: List[str], act: str = \"gelu\"):\n",
        "        super(AudioEncoder, self).__init__()\n",
        "\n",
        "        self.dims = dims\n",
        "        self.head = head\n",
        "        self.ctx = ctx\n",
        "        self.head_dim = dims // head\n",
        "        self.debug = debug\n",
        "        self._counter = 0\n",
        "        self.features = features\n",
        "        self.dropout = 0.01\n",
        "\n",
        "        act_map = {\"gelu\": nn.GELU(), \"relu\": nn.ReLU(), \"sigmoid\": nn.Sigmoid(), \"tanh\": nn.Tanh(), \"swish\": nn.SiLU(),\"tanhshrink\": nn.Tanhshrink(), \"softplus\": nn.Softplus(), \"softshrink\": nn.Softshrink(), \"leaky_relu\": nn.LeakyReLU(), \"elu\": nn.ELU()}\n",
        "        act_fn = act_map.get(act, nn.GELU())\n",
        "\n",
        "        if features == [\"spectrogram\", \"waveform\", \"pitch\"]:\n",
        "            cgate=True\n",
        "        else:\n",
        "            cgate = False\n",
        "\n",
        "        self.blocks = nn.ModuleDict({\n",
        "            \"spectrogram\": nn.ModuleList(\n",
        "            [FEncoder(input_dims=mels, dims=dims, head=head, layer=layer, kernel_size=3, act=act_fn)] +\n",
        "            [Residual(ctx=ctx, dims=dims, head=head, act=act, debug=debug, features=features, cgate=cgate) for _ in range(layer)] if \"spectrogram\" in features else None\n",
        "            ),\n",
        "            \"waveform\": nn.ModuleList(\n",
        "            [WEncoder(input_dims=1, dims=dims, head=head, layer=layer, kernel_size=11, act=act_fn)] +\n",
        "            [Residual(ctx=ctx, dims=dims, head=head, act=act, debug=debug, features=features, cgate=cgate) for _ in range(layer)] if \"waveform\" in features else None\n",
        "            ),\n",
        "            \"pitch\": nn.ModuleList(\n",
        "            [FEncoder(input_dims=1, dims=dims, head=head, layer=layer, kernel_size=9, act=act, stride=2)] +\n",
        "            [Residual(ctx=ctx, dims=dims, head=head, act=act, debug=debug, features=features, cgate=cgate) for _ in range(layer)] if \"pitch\" in features else None\n",
        "            ),\n",
        "            \"envelope\": nn.ModuleList(\n",
        "            [FEncoder(input_dims=mels, dims=dims, head=head, layer=layer, kernel_size=3, act=act_fn)] +\n",
        "            [Residual(ctx=ctx, dims=dims, head=head, act=act, debug=debug, features=features, cgate=cgate)\n",
        "             for _ in range(layer)] if \"envelope\" in features else None\n",
        "            ),\n",
        "            \"phase\": nn.ModuleList(\n",
        "            [FEncoder(input_dims=mels, dims=dims, head=head, layer=layer, kernel_size=3, act=act_fn)] +\n",
        "            [Residual(ctx=ctx, dims=dims, head=head, act=act, debug=debug, features=features, cgate=cgate)\n",
        "            for _ in range(layer)] if \"phase\" in features else None\n",
        "        )})\n",
        "\n",
        "    def forward(self, enc, layer=\"encoder\"):\n",
        "        enc = dict_to(enc, device, dtype)\n",
        "\n",
        "        if self._counter < 1:\n",
        "            s = enc.get(\"spectrogram\")\n",
        "            w = enc.get(\"waveform\")\n",
        "            p = default(enc.get(\"pitch\"), enc.get(\"f0\"))\n",
        "            plot_waveform(x=s, w=w, p=p, hop_length=128)\n",
        "\n",
        "        xa = {}\n",
        "\n",
        "        for f in self.features:\n",
        "            if f in enc and f in self.blocks:\n",
        "                x = enc[f]\n",
        "                for block in self.blocks[f]:\n",
        "                    x = block(x, enc=enc, layer=layer)\n",
        "                xa[f] = x\n",
        "\n",
        "        if \"encoder\" in self.debug and self._counter % 100 == 0:\n",
        "            names = list(x.keys())\n",
        "            shapes = {k: v.shape for k, v in x.items()}\n",
        "            print(f\"Step {self._counter}: mode: {names}: shapes: {shapes}\")\n",
        "        self._counter += 1\n",
        "        return xa\n",
        "\n",
        "class TextDecoder(nn.Module):\n",
        "    def __init__(self, vocab: int, ctx: int, dims: int, head: int, layer: int, cross_attn: bool,\n",
        "                debug: List[str], features: List[str], sequential=False):\n",
        "        super(TextDecoder, self).__init__()\n",
        "\n",
        "        self.ctx = ctx\n",
        "        self.dims = dims\n",
        "        self.head = head\n",
        "        self.head_dim = dims // head\n",
        "        self.debug = debug\n",
        "        self._counter = 0\n",
        "        self.dropout = 0.01\n",
        "        self.sequential = sequential\n",
        "        self.features = features\n",
        "\n",
        "        self.token = nn.Embedding(num_embeddings=vocab, embedding_dim=dims)\n",
        "        with torch.no_grad():\n",
        "            self.token.weight[0].zero_()\n",
        "        self.positional = nn.Parameter(data=torch.empty(ctx, dims), requires_grad=True)\n",
        "\n",
        "        self.block = nn.ModuleList([\n",
        "            Residual(ctx=ctx, dims=dims, head=head, act=\"gelu\", cross_attn=cross_attn, debug=debug, features=features)\n",
        "            for _ in range(layer)])\n",
        "\n",
        "        self.blocks = nn.ModuleDict({\n",
        "        f: nn.ModuleList([Residual(ctx=ctx, dims=dims, head=head, act=\"gelu\", cross_attn=cross_attn, debug=debug, features=features)\n",
        "            for _ in range(layer)]) for f in features})\n",
        "\n",
        "        self.blend = nn.ParameterDict({f: nn.Parameter(torch.tensor(0.5)) for f in features})\n",
        "        self.ln_dec = RMSNorm(dims)\n",
        "\n",
        "        mask = torch.tril(torch.ones(ctx, ctx), diagonal=0)\n",
        "        self.register_buffer(\"mask\", mask, persistent=False)\n",
        "\n",
        "    def forward(self, x, xa, enc=None, order=None, layer='decoder') -> Tensor:\n",
        "        xa = dict_to(xa, device, dtype)\n",
        "        x = x.to(device)\n",
        "        bln = self.blend\n",
        "\n",
        "        if order is None:\n",
        "            order = self.features\n",
        "\n",
        "        mask = self.mask[:x.shape[1], :x.shape[1]]\n",
        "        x = self.token(x) + self.positional[:x.shape[1]]\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        for block in self.block:\n",
        "            x = block(x, xa=None, mask=mask, enc=enc, layer=layer)\n",
        "\n",
        "        for f in order:\n",
        "            if f in xa:\n",
        "                ax = xa[f]\n",
        "                for block in self.blocks[f]:\n",
        "                    out = block(x=x, xa=ax, mask=None, enc=enc, layer=layer)\n",
        "\n",
        "                a = torch.sigmoid(bln[f])\n",
        "                x = a * out + (1 - a) * x\n",
        "\n",
        "        if \"decoder\" in self.debug and self._counter % 100 == 0:\n",
        "            print(f\"Step {self._counter}: Decoder output shape: {x.shape}, enc keys: {list(enc.keys())}, order: {order}\")\n",
        "        self._counter += 1\n",
        "\n",
        "        x = self.ln_dec(x)\n",
        "        return x @ torch.transpose(self.token.weight.to(dtype), 0, 1).float()\n",
        "\n",
        "class Echo(nn.Module):\n",
        "    def __init__(self, param: Dimensions):\n",
        "        super().__init__()\n",
        "        self.param = param\n",
        "        self.count = 0\n",
        "\n",
        "        self.encoder = AudioEncoder(\n",
        "            mels=param.mels,\n",
        "            ctx=param.aud_ctx,\n",
        "            dims=param.aud_dims,\n",
        "            head=param.aud_head,\n",
        "            layer=param.aud_idx,\n",
        "            act=param.act,\n",
        "            debug=param.debug,\n",
        "            features=param.features,\n",
        "            )\n",
        "\n",
        "        self.decoder = TextDecoder(\n",
        "            vocab=param.vocab,\n",
        "            ctx=param.text_ctx,\n",
        "            dims=param.text_dims,\n",
        "            head=param.text_head,\n",
        "            layer=param.text_idx,\n",
        "            cross_attn=param.cross_attn,\n",
        "            debug=param.debug,\n",
        "            features=param.features,\n",
        "            )\n",
        "\n",
        "        all_head = torch.zeros(self.param.text_idx, self.param.text_head, dtype=torch.bool)\n",
        "        all_head[self.param.text_idx // 2 :] = True\n",
        "        self.register_buffer(\"alignment_head\", all_head.to_sparse(), persistent=False)\n",
        "\n",
        "    def update_base(self, f0):\n",
        "        for name, module in self.encoder.named_modules():\n",
        "            if isinstance(module, (rotary)):\n",
        "                module.update_base(f0)\n",
        "                module.return_f0(f0)\n",
        "\n",
        "        for name, module in self.decoder.named_modules():\n",
        "            if isinstance(module, (rotary)):\n",
        "                module.update_base(f0)\n",
        "                module.return_f0(f0)\n",
        "\n",
        "    def set_alignment_head(self, dump: bytes):\n",
        "        array = np.frombuffer(\n",
        "            gzip.decompress(base64.b85decode(dump)), dtype=bool).copy()\n",
        "        mask = torch.from_numpy(array).reshape(\n",
        "            self.param.text_idx, self.param.text_head)\n",
        "        self.register_buffer(\"alignment_head\", mask.to_sparse(), persistent=False)\n",
        "\n",
        "    def embed_audio(self, spectrogram: torch.Tensor):\n",
        "        return self.encoder(spectrogram)\n",
        "\n",
        "    def logits(self,input_ids: torch.Tensor, encoder_output: torch.Tensor):\n",
        "        return self.decoder(input_ids, encoder_output)\n",
        "\n",
        "    def forward(self,\n",
        "        decoder_input_ids=None,\n",
        "        labels=None,\n",
        "        waveform: Optional[torch.Tensor]=None,\n",
        "        input_ids=None,\n",
        "        spectrogram: torch.Tensor=None,\n",
        "        pitch: Optional[torch.Tensor]=None,\n",
        "        f0: Optional[torch.Tensor]=None,\n",
        "        f0d: Optional[torch.Tensor]=None,\n",
        "        envelope: Optional[torch.Tensor]=None,\n",
        "        phase: Optional[torch.Tensor]=None,\n",
        "        ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        decoder_input_ids = input_ids\n",
        "        encoder_inputs = {}\n",
        "        if spectrogram is not None:\n",
        "            encoder_inputs[\"spectrogram\"] = spectrogram\n",
        "        if waveform is not None:\n",
        "            encoder_inputs[\"waveform\"] = waveform\n",
        "        if pitch is not None:\n",
        "            encoder_inputs[\"pitch\"] = pitch\n",
        "        if envelope is not None:\n",
        "            encoder_inputs[\"envelope\"] = envelope\n",
        "        if phase is not None:\n",
        "            encoder_inputs[\"phase\"] = phase\n",
        "        if f0 is not None:\n",
        "            encoder_inputs[\"f0\"] = f0\n",
        "\n",
        "        if f0 is not None:\n",
        "            f0 = f0.squeeze(0)\n",
        "            self.update_base(f0)\n",
        "\n",
        "        encoder_outputs = self.encoder(encoder_inputs)\n",
        "        logits = self.decoder(input_ids, encoder_outputs)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.shape[-1]), labels.view(-1), ignore_index=0)\n",
        "\n",
        "        self.count += 1\n",
        "        return {\n",
        "            \"logits\": logits,\n",
        "            \"loss\": loss,\n",
        "            \"labels\": labels,\n",
        "            \"input_ids\": input_ids,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"encoder_output\": encoder_outputs,\n",
        "            }\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "    @property\n",
        "    def dtype(self):\n",
        "        return next(self.parameters()).dtype\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = 0.02\n",
        "        self.init_counts = {\n",
        "            \"Linear\": 0, \"Conv1d\": 0, \"LayerNorm\": 0, \"RMSNorm\": 0,\n",
        "            \"Conv2d\": 0, \"SEBlock\": 0, \"TextDecoder\": 0, \"AudioEncoder\": 0,\n",
        "            \"Residual\": 0, \"MultiheadA\": 0, \"MultiheadB - Cross Attention\": 0,\n",
        "            \"MultiheadC\": 0, \"MultiheadD\": 0, \"FEncoder\": 0,\n",
        "            \"WEncoder\": 0, \"PEncoder\": 0}\n",
        "\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, RMSNorm):\n",
        "                nn.init.ones_(module.weight)\n",
        "                self.init_counts[\"RMSNorm\"] += 1\n",
        "            elif isinstance(module, nn.Linear):\n",
        "                if module.weight is not None:\n",
        "                    nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "                self.init_counts[\"Linear\"] += 1\n",
        "            elif isinstance(module, Conv1d):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "                self.init_counts[\"Conv1d\"] += 1\n",
        "            elif isinstance(module, Conv2d):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "                self.init_counts[\"Conv2d\"] += 1\n",
        "            elif isinstance(module, MultiheadA):\n",
        "\n",
        "                self.init_counts[\"MultiheadA\"] += 1\n",
        "            elif isinstance(module, TextDecoder):\n",
        "                self.init_counts[\"TextDecoder\"] += 1\n",
        "            elif isinstance(module, AudioEncoder):\n",
        "                self.init_counts[\"AudioEncoder\"] += 1\n",
        "            elif isinstance(module, Residual):\n",
        "                self.init_counts[\"Residual\"] += 1\n",
        "\n",
        "    def init_weights(self):\n",
        "        print(\"Initializing model weights...\")\n",
        "        self.apply(self._init_weights)\n",
        "        print(\"Initialization summary:\")\n",
        "        for module_type, count in self.init_counts.items():\n",
        "            if count > 0:\n",
        "                print(f\"{module_type}: {count}\")\n",
        "\n",
        "    def register_gradient_hooks(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                if \"encoder\" in name:\n",
        "                    param.register_hook(lambda grad, n=name: self._print_encoder_grad(n, grad))\n",
        "                elif \"decoder\" in name:\n",
        "                    param.register_hook(lambda grad, n=name: self._print_decoder_grad(n, grad))\n",
        "\n",
        "        print(\"Gradient debugging hooks registered\")\n",
        "        return self\n",
        "\n",
        "    def _print_encoder_grad(self, name, grad):\n",
        "        if grad is not None and self.count == 10:\n",
        "            norm = grad.median().item()\n",
        "            print(f\"ENCODER GRAD: {name} = {norm:.6f}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _print_decoder_grad(self, name, grad):\n",
        "        if grad is not None and self.count == 10:\n",
        "            norm = grad.median().item()\n",
        "            print(f\"DECODER GRAD: {name} = {norm:.6f}\")\n",
        "        return None\n",
        "\n",
        "    def reset_counter(self):\n",
        "        self._counter = 0\n",
        "        print(\"Counter reset to 0.\")\n",
        "\n",
        "metric = evaluate.load(path=\"wer\")\n",
        "\n",
        "def align_f0(f0, ctx):\n",
        "    ctx = torch.tensor(ctx)\n",
        "    bat, length = f0.shape\n",
        "    if length == ctx:\n",
        "        return f0\n",
        "    frames = length / ctx\n",
        "    idx = torch.arange(ctx, device=f0.device)\n",
        "    idx = (idx * frames).long()\n",
        "    batch_idx = torch.arange(bat, device=f0.device).unsqueeze(1)\n",
        "    return f0[batch_idx, idx.unsqueeze(0).expand(bat, -1)]\n",
        "\n",
        "@dataclass\n",
        "class DataCollator:\n",
        "    tokenizer: Any\n",
        "    def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
        "        pad_token_id = tokenizer.pad_token_id if hasattr(tokenizer, 'pad_token_id') else 0\n",
        "        bos_token_id = tokenizer.bos_token_id if hasattr(tokenizer, 'bos_token_id') else 1\n",
        "\n",
        "        batch = {}\n",
        "\n",
        "        if \"spectrogram\" in features[0] and features[0][\"spectrogram\"] is not None:\n",
        "            spectrogram_list = [f[\"spectrogram\"] for f in features]\n",
        "            max_len_feat = max(f.shape[-1] for f in spectrogram_list)\n",
        "            pad_spectrogram = []\n",
        "            for feat in spectrogram_list:\n",
        "                current_len = feat.shape[-1]\n",
        "                padding = max_len_feat - current_len\n",
        "                if padding > 0:\n",
        "                    pad_feat = F.pad(feat, (0, padding), mode='constant', value=pad_token_id)\n",
        "                else:\n",
        "                    pad_feat = feat\n",
        "                pad_spectrogram.append(pad_feat)\n",
        "            batch[\"spectrogram\"] = torch.stack(pad_spectrogram)\n",
        "\n",
        "        if \"waveform\" in features[0] and features[0][\"waveform\"] is not None:\n",
        "            waveform_list = [f[\"waveform\"] for f in features]\n",
        "            max_len_wav = max(w.shape[-1] for w in waveform_list)\n",
        "            pad_waveforms = []\n",
        "            for wav in waveform_list:\n",
        "                current_len = wav.shape[-1]\n",
        "                padding = max_len_wav - current_len\n",
        "                if padding > 0:\n",
        "                    if wav.ndim == 1:\n",
        "                        wav = wav.unsqueeze(0)\n",
        "                    pad_wav = F.pad(wav, (0, padding), mode='constant', value=pad_token_id)\n",
        "                else:\n",
        "                    pad_wav = wav\n",
        "                pad_waveforms.append(pad_wav)\n",
        "            batch[\"waveform\"] = torch.stack(pad_waveforms)\n",
        "\n",
        "        if \"label\" in features[0] and features[0][\"label\"] is not None:\n",
        "            labels_list = [f[\"label\"] for f in features]\n",
        "            max_len = max(len(l) for l in labels_list)\n",
        "            all_ids = []\n",
        "            all_labels = []\n",
        "\n",
        "            for label in labels_list:\n",
        "                label_list = label.tolist() if isinstance(label, torch.Tensor) else label\n",
        "                decoder_input = [bos_token_id] + label_list\n",
        "                label_eos = label_list + [pad_token_id]\n",
        "                input_len = max_len + 1 - len(decoder_input)\n",
        "                label_len = max_len + 1 - len(label_eos)\n",
        "                padded_input = decoder_input + [pad_token_id] * input_len\n",
        "                padded_labels = label_eos + [pad_token_id] * label_len\n",
        "                all_ids.append(padded_input)\n",
        "                all_labels.append(padded_labels)\n",
        "            batch[\"input_ids\"] = torch.tensor(all_ids, dtype=torch.long)\n",
        "            batch[\"labels\"] = torch.tensor(all_labels, dtype=torch.long)\n",
        "\n",
        "        if \"pitch\" in features[0] and features[0][\"pitch\"] is not None:\n",
        "            pitch_list = [f[\"pitch\"] for f in features]\n",
        "            max_len_pitch = max(e.shape[-1] for e in pitch_list)\n",
        "            pad_pitch = []\n",
        "            for pitch in pitch_list:\n",
        "                current_len = pitch.shape[-1]\n",
        "                padding = max_len_pitch - current_len\n",
        "                if padding > 0:\n",
        "                    pad_pitch_item = F.pad(pitch, (0, padding), mode='constant', value=pad_token_id)\n",
        "                else:\n",
        "                    pad_pitch_item = pitch\n",
        "                pad_pitch.append(pad_pitch_item)\n",
        "            batch[\"pitch\"] = torch.stack(pad_pitch)\n",
        "\n",
        "        if \"f0\" in features[0] and features[0][\"f0\"] is not None:\n",
        "            f0_list = [f[\"f0\"] for f in features]\n",
        "            max_len_f0 = max(f.shape[-1] for f in f0_list)\n",
        "            pad_f0 = []\n",
        "            for f0 in f0_list:\n",
        "                current_len = f0.shape[-1]\n",
        "                padding = max_len_f0 - current_len\n",
        "                if padding > 0:\n",
        "                    pad_f0_item = F.pad(f0, (0, padding), mode='constant', value=pad_token_id)\n",
        "                else:\n",
        "                    pad_f0_item = f0\n",
        "                pad_f0.append(pad_f0_item)\n",
        "            batch[\"f0\"] = torch.stack(pad_f0)\n",
        "\n",
        "        if \"envelope\" in features[0] and features[0][\"envelope\"] is not None:\n",
        "            env_list = [f[\"envelope\"] for f in features]\n",
        "            max_len = max(f.shape[-1] for f in env_list)\n",
        "            pad_env = []\n",
        "            for feat in env_list:\n",
        "                current_len = feat.shape[-1]\n",
        "                padding = max_len - current_len\n",
        "                if padding > 0:\n",
        "                    pad_feat = F.pad(feat, (0, padding), mode='constant', value=pad_token_id)\n",
        "                else:\n",
        "                    pad_feat = feat\n",
        "                pad_env.append(pad_feat)\n",
        "            batch[\"envelope\"] = torch.stack(pad_env)\n",
        "\n",
        "        if \"phase\" in features[0] and features[0][\"phase\"] is not None:\n",
        "            ph_list = [f[\"phase\"] for f in features]\n",
        "            max_len = max(f.shape[-1] for f in ph_list)\n",
        "            pad_ph = []\n",
        "            for feat in ph_list:\n",
        "                current_len = feat.shape[-1]\n",
        "                padding = max_len - current_len\n",
        "                if padding > 0:\n",
        "                    pad_feat = F.pad(feat, (0, padding), mode='constant', value=pad_token_id)\n",
        "                else:\n",
        "                    pad_feat = feat\n",
        "                pad_ph.append(pad_feat)\n",
        "            batch[\"phase\"] = torch.stack(pad_ph)\n",
        "        return batch\n",
        "\n",
        "def hilbert_transform(x):\n",
        "    N = x.shape[-1]\n",
        "    xf = torch.fft.rfft(x)\n",
        "    h = torch.zeros(N // 2 + 1, device=x.device, dtype=x.dtype)\n",
        "    if N % 2 == 0:\n",
        "        h[0] = h[N//2] = 1\n",
        "        h[1:N//2] = 2\n",
        "    else:\n",
        "        h[0] = 1\n",
        "        h[1:(N+1)//2] = 2\n",
        "    return torch.fft.irfft(xf * h, n=N)\n",
        "\n",
        "def analytic_signal(x):\n",
        "    return x + 1j * hilbert_transform(x)\n",
        "\n",
        "def hilbert_transform_2d(x, dim=-1):\n",
        "    N = x.shape[dim]\n",
        "    if dim == -1 or dim == len(x.shape) - 1:\n",
        "        xf = torch.fft.rfft(x)\n",
        "    else:\n",
        "        xf = torch.fft.rfft(x, dim=dim)\n",
        "    h_shape = [1] * len(x.shape)\n",
        "    h_shape[dim] = N // 2 + 1\n",
        "    h = torch.zeros(h_shape, device=x.device, dtype=x.dtype)\n",
        "    if dim == -1 or dim == len(x.shape) - 1:\n",
        "        if N % 2 == 0:\n",
        "            h[..., 0] = h[..., -1] = 1\n",
        "            h[..., 1:-1] = 2\n",
        "        else:\n",
        "            h[..., 0] = 1\n",
        "            h[..., 1:] = 2\n",
        "    else:\n",
        "        pass\n",
        "    return torch.fft.irfft(xf * h, n=N, dim=dim)\n",
        "\n",
        "def hilbert_transform_true_2d(x):\n",
        "    xf = torch.fft.rfft2(x)\n",
        "    h1, h2 = torch.meshgrid(\n",
        "        torch.fft.rfftfreq(x.shape[-2]) * 2 - 1,\n",
        "        torch.fft.rfftfreq(x.shape[-1]) * 2 - 1,\n",
        "        indexing='ij')\n",
        "    h = -1j / (math.pi * (h1 + 1j*h2))\n",
        "    h[0, 0] = 0\n",
        "    return torch.fft.irfft2(xf * h.to(x.device))\n",
        "\n",
        "def process_spectrogram_with_hilbert(spec):\n",
        "    analytic = spec + 1j * hilbert_transform(spec)\n",
        "    envelope = torch.abs(analytic)\n",
        "    phase = torch.angle(analytic)\n",
        "    return envelope, phase\n",
        "\n",
        "def load_wave(wave_data, sample_rate):\n",
        "    if isinstance(wave_data, str):\n",
        "        waveform, sr = torchaudio.load(uri=wave_data, normalize=False)\n",
        "    elif isinstance(wave_data, dict):\n",
        "        waveform = torch.tensor(data=wave_data[\"array\"]).float()\n",
        "        sr = wave_data[\"sampling_rate\"]\n",
        "    else:\n",
        "        raise TypeError(\"Invalid wave_data format.\")\n",
        "\n",
        "    if waveform.dim() == 1:\n",
        "        waveform = waveform.unsqueeze(0)\n",
        "\n",
        "    if sr != sample_rate:\n",
        "        original_length = waveform.shape[1]\n",
        "        target_length = int(original_length * (sample_rate / sr))\n",
        "\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    return waveform.flatten()\n",
        "\n",
        "def extract_features(batch, tokenizer, spectrogram, waveforms, pitch, frequency=False,\n",
        "                     hop_length=128, fmin=0, fmax=8000, n_mels=128, n_fft=1024, sampling_rate=16000,\n",
        "                     pad_mode=\"constant\", center=True, power=2.0, window_fn=torch.hann_window, mel_scale=\"htk\",\n",
        "                     norm=None, normalized=False, downsamples=False, period=False, hilbert=False):\n",
        "\n",
        "    dtype = torch.float32\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    audio = batch[\"audio\"]\n",
        "    sampling_rate = audio[\"sampling_rate\"]\n",
        "    sr = audio[\"sampling_rate\"]\n",
        "    wav = load_wave(wave_data=audio, sample_rate=sr)\n",
        "\n",
        "    if spectrogram:\n",
        "        transform = torchaudio.transforms.MelSpectrogram(\n",
        "            f_max=fmax,\n",
        "            f_min=fmin,\n",
        "            n_mels=n_mels,\n",
        "            sample_rate=sr,\n",
        "            n_fft=n_fft,\n",
        "            hop_length=hop_length,\n",
        "            norm=norm,\n",
        "            normalized=normalized,\n",
        "            power=power,\n",
        "            center=center,\n",
        "            mel_scale=mel_scale,\n",
        "            window_fn=window_fn,\n",
        "            pad_mode=pad_mode)\n",
        "\n",
        "        mel_spectrogram = transform(wav)\n",
        "        log_mel = torch.clamp(mel_spectrogram, min=1e-10).log10()\n",
        "        log_mel = torch.maximum(log_mel, log_mel.max() - 8.0)\n",
        "        spec = (log_mel + 4.0) / 4.0\n",
        "        spec = torch.tensor(spec)\n",
        "        batch[\"spectrogram\"] = spec\n",
        "\n",
        "    if hilbert:\n",
        "        envelope_list = []\n",
        "        phase_list = []\n",
        "\n",
        "        for ch_idx in range(spec.shape[0]):\n",
        "            envelope, phase = process_spectrogram_with_hilbert(spec[ch_idx])\n",
        "            envelope_list.append(envelope)\n",
        "            phase_list.append(phase)\n",
        "\n",
        "        batch[\"envelope\"] = torch.stack(envelope_list)\n",
        "        batch[\"phase\"] = torch.stack(phase_list)\n",
        "\n",
        "    wav_1d = wav.unsqueeze(0)\n",
        "\n",
        "    if waveforms:\n",
        "        batch[\"waveform\"] = wav_1d\n",
        "\n",
        "    if pitch:\n",
        "        wav_np = wav.numpy().astype(np.float64)\n",
        "        f0, t = pw.dio(wav_np, sampling_rate,\n",
        "                    frame_period=hop_length/sampling_rate*1000)\n",
        "        f0 = pw.stonemask(wav_np, f0, t, sampling_rate)\n",
        "        f0 = torch.from_numpy(f0).float()\n",
        "        batch[\"pitch\"] = f0\n",
        "\n",
        "    if frequency:\n",
        "        wav_np = wav.numpy().astype(np.float64)\n",
        "        f0, t = pw.dio(wav_np, sampling_rate, frame_period=hop_length/sampling_rate*1000)\n",
        "        f0 = pw.stonemask(wav_np, f0, t, sampling_rate)\n",
        "        f0 = torch.from_numpy(f0).float()\n",
        "        batch[\"f0\"] = f0\n",
        "\n",
        "    if spectrogram and waveforms and pitch:\n",
        "        spec_mean = batch[\"spectrogram\"].mean()\n",
        "        spec_std = batch[\"spectrogram\"].std() + 1e-6\n",
        "        batch[\"spectrogram\"] = (batch[\"spectrogram\"] - spec_mean) / spec_std\n",
        "\n",
        "        wav_mean = batch[\"waveform\"].mean()\n",
        "        wav_std = batch[\"waveform\"].std() + 1e-6\n",
        "        batch[\"waveform\"] = (batch[\"waveform\"] - wav_mean) / wav_std\n",
        "\n",
        "        if batch[\"pitch\"].max() > 1.0:\n",
        "            pitch_min = 50.0\n",
        "            pitch_max = 500.0\n",
        "            batch[\"pitch\"] = (batch[\"pitch\"] - pitch_min) / (pitch_max - pitch_min)\n",
        "\n",
        "    batch[\"label\"] = tokenizer.encode(batch[\"transcription\"], add_special_tokens=False)\n",
        "    return batch\n",
        "\n",
        "def compute_metrics(eval_pred, compute_result: bool = True,\n",
        "                    print_pred: bool = False, num_samples: int = 0, tokenizer=None, pitch=None, model=None):\n",
        "\n",
        "    pred_logits = eval_pred.predictions\n",
        "    label_ids = eval_pred.label_ids\n",
        "\n",
        "    if hasattr(pred_logits, \"cpu\"):\n",
        "        pred_logits = pred_logits.cpu()\n",
        "    if hasattr(label_ids, \"cpu\"):\n",
        "        label_ids = label_ids.cpu()\n",
        "    if isinstance(pred_logits, tuple):\n",
        "        pred_ids = pred_logits[0]\n",
        "    else:\n",
        "        pred_ids = pred_logits\n",
        "    if hasattr(pred_ids, \"ndim\") and pred_ids.ndim == 3:\n",
        "        if not isinstance(pred_ids, torch.Tensor):\n",
        "            pred_ids = torch.tensor(pred_ids)\n",
        "        pred_ids = pred_ids.argmax(dim=-1)\n",
        "        pred_ids = pred_ids.tolist()\n",
        "\n",
        "    if hasattr(label_ids, \"tolist\"):\n",
        "        label_ids = label_ids.tolist()\n",
        "\n",
        "    label_ids = [[0 if token == -100 else token for token in seq] for seq in label_ids]\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=False)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=False)\n",
        "\n",
        "    if print_pred:\n",
        "        for i in range(min(num_samples, len(pred_str))):\n",
        "            print(f\"Preds: {pred_str[i]}\")\n",
        "            print(f\"Label: {label_str[i]}\")\n",
        "            print(f\"preds: {pred_ids[i]}\")\n",
        "            print(f\"label: {label_ids[i]}\")\n",
        "            print(\"--------------------------------\")\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    if model is None:\n",
        "        global global_model\n",
        "        if 'global_model' in globals():\n",
        "            model = global_model\n",
        "\n",
        "    if model is not None:\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1_000_000\n",
        "        if trainable_params > 0:\n",
        "            efficiency_score = (100 - wer) / trainable_params\n",
        "        else:\n",
        "            print(\"Warning: Zero trainable parameters detected\")\n",
        "            efficiency_score = 0.0\n",
        "    else:\n",
        "        print(\"Warning: Model not available for parameter counting\")\n",
        "        trainable_params = 0.0\n",
        "        efficiency_score = 0.0\n",
        "\n",
        "    if hasattr(wer, \"item\"):\n",
        "        wer = wer.item()\n",
        "\n",
        "    metrics = {\n",
        "        \"wer\": float(wer),\n",
        "        \"trainable_params_M\": float(trainable_params),\n",
        "        \"efficiency_score\": float(efficiency_score),\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def create_model(param: Dimensions) -> Echo:\n",
        "    model = Echo(param).to('cuda')\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    logger.info(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def setup_tokenizer(token: str, local_tokenizer_path: str = \"D:/newmodel/model/tokenn/\"):\n",
        "    from tokenizers import Tokenizer\n",
        "    tokenizer = Tokenizer.from_file(f\"{local_tokenizer_path}/tokenizer.json\")\n",
        "    orig_encode = tokenizer.encode\n",
        "    def enc(text, add_special_tokens=True):\n",
        "        ids = orig_encode(text).ids\n",
        "        if not add_special_tokens:\n",
        "            sp_ids = [tokenizer.token_to_id(t) for t in [\"<PAD>\", \"<BOS>\", \"<EOS>\"]]\n",
        "            ids = [id for id in ids if id not in sp_ids]\n",
        "        return ids\n",
        "    def bdec(ids_list, skip_special_tokens=True):\n",
        "        results = []\n",
        "        for ids in ids_list:\n",
        "            if skip_special_tokens:\n",
        "                ids = [id for id in ids if id not in [0, 1, 2]]\n",
        "            results.append(tokenizer.decode(ids))\n",
        "        return results\n",
        "    def save_pretrained(save_dir):\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        tokenizer.save(f\"{save_dir}/tokenizer.json\")\n",
        "    tokenizer.encode = enc\n",
        "    tokenizer.batch_decode = bdec\n",
        "    tokenizer.save_pretrained = save_pretrained\n",
        "    tokenizer.pad_token_id = 0\n",
        "    tokenizer.bos_token_id = 1\n",
        "    tokenizer.eos_token_id = 2\n",
        "    return tokenizer\n",
        "\n",
        "def prepare_datasets(tokenizer, token: str, sanity_check: bool = False, dataset_config: Optional[Dict] = None) -> Tuple[any, any]:\n",
        "    if dataset_config is None:\n",
        "        dataset_config = {\n",
        "            \"spectrogram\": True,\n",
        "            \"waveforms\": True,\n",
        "            \"pitch\": True,\n",
        "            \"frequency\": True,\n",
        "            \"downsamples\": True,\n",
        "            \"hop_length\": 128,\n",
        "            \"fmin\": 50,\n",
        "            \"fmax\": 2000,\n",
        "            \"n_mels\": 128,\n",
        "            \"n_fft\": 1024,\n",
        "            \"sampling_rate\": 16000,\n",
        "        }\n",
        "\n",
        "    dataset = load_dataset(\n",
        "        \"google/fleurs\",\n",
        "        \"en_us\",\n",
        "        token=token,\n",
        "        trust_remote_code=True,\n",
        "        streaming=False)\n",
        "\n",
        "    dataset = dataset.cast_column(column=\"audio\", feature=Audio(sampling_rate=16000)).select_columns([\"audio\", \"transcription\"])\n",
        "\n",
        "    if sanity_check:\n",
        "        dataset = dataset[\"test\"].take(10)\n",
        "        dataset = dataset.select_columns([\"audio\", \"transcription\"])\n",
        "        logger.info(f\"Sanity dataset size: {dataset.num_rows}\")\n",
        "        print(f\"Sanity dataset size: {dataset.num_rows}\")\n",
        "        prepare_fn = partial(extract_features, tokenizer=tokenizer, **dataset_config)\n",
        "\n",
        "        dataset = dataset.map(\n",
        "            function=prepare_fn,\n",
        "            remove_columns=[\"audio\", \"transcription\"]\n",
        "        ).with_format(type=\"torch\")\n",
        "        train_dataset = dataset\n",
        "        test_dataset = dataset\n",
        "    else:\n",
        "        def filter_func(x):\n",
        "            return (0 < len(x[\"transcription\"]) < 512 and\n",
        "                   len(x[\"audio\"][\"array\"]) > 0 and\n",
        "                   len(x[\"audio\"][\"array\"]) < 1500 * 160)\n",
        "\n",
        "        dataset = dataset.filter(filter_func).shuffle(seed=4)\n",
        "        logger.info(f\"Dataset size: {dataset['train'].num_rows}, {dataset['test'].num_rows}\")\n",
        "        print(f\"Dataset size: {dataset['train'].num_rows}, {dataset['test'].num_rows}\")\n",
        "        prepare_fn = partial(extract_features, tokenizer=tokenizer, **dataset_config)\n",
        "        columns_to_remove = list(next(iter(dataset.values())).features)\n",
        "        train_dataset = dataset[\"train\"]\n",
        "        test_dataset = dataset[\"test\"].take(50)\n",
        "        logger.info(f\"Train dataset size: {train_dataset.num_rows}, Test dataset size: {test_dataset.num_rows}\")\n",
        "\n",
        "        train_dataset = train_dataset.map(\n",
        "            function=prepare_fn,\n",
        "            remove_columns=columns_to_remove\n",
        "        ).with_format(type=\"torch\")\n",
        "\n",
        "        test_dataset = test_dataset.map(\n",
        "            function=prepare_fn,\n",
        "            remove_columns=columns_to_remove\n",
        "        ).with_format(type=\"torch\")\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def get_training_args(\n",
        "    log_dir: str,\n",
        "    batch_eval_metrics: bool = False,\n",
        "    max_steps: int = 10,\n",
        "    save_steps: int = 1000,\n",
        "    eval_steps: int = 1,\n",
        "    warmup_steps: int = 0,\n",
        "    num_train_epochs: int = 1,\n",
        "    logging_steps: int = 1,\n",
        "    eval_on_start: bool = False,\n",
        "    learning_rate: float = 1e-4,\n",
        "    weight_decay: float = 0.01,\n",
        "    max_grad_norm: float = 1.0,\n",
        ") -> Seq2SeqTrainingArguments:\n",
        "\n",
        "    return Seq2SeqTrainingArguments(\n",
        "        output_dir=log_dir,\n",
        "        per_device_train_batch_size=1,\n",
        "        per_device_eval_batch_size=1,\n",
        "        gradient_accumulation_steps=1,\n",
        "        eval_accumulation_steps=1,\n",
        "        eval_strategy=\"steps\",\n",
        "        save_strategy=\"no\",\n",
        "        include_tokens_per_second=True,\n",
        "        include_num_input_tokens_seen=True,\n",
        "        max_steps=max_steps,\n",
        "        save_steps=save_steps,\n",
        "        eval_steps=eval_steps,\n",
        "        warmup_steps=warmup_steps,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        logging_steps=logging_steps,\n",
        "        logging_dir=log_dir,\n",
        "        logging_strategy=\"steps\",\n",
        "        report_to=[\"tensorboard\"],\n",
        "        push_to_hub=False,\n",
        "        disable_tqdm=False,\n",
        "        save_total_limit=1,\n",
        "        label_names=[\"labels\"],\n",
        "        optim=\"adamw_torch\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        save_safetensors=False,\n",
        "        eval_on_start=eval_on_start,\n",
        "        batch_eval_metrics=batch_eval_metrics,\n",
        "        max_grad_norm=max_grad_norm,\n",
        "    )\n",
        "\n",
        "def main():\n",
        "\n",
        "    token = \"\"\n",
        "    log_dir = os.path.join('./output/logs', datetime.now().strftime(format='%m-%d_%H_%M_%S'))\n",
        "    os.makedirs(name=log_dir, exist_ok=True)\n",
        "    tokenizer = setup_tokenizer(token)\n",
        "\n",
        "    def sanity(sanity: bool):\n",
        "\n",
        "        if sanity:\n",
        "            training_args = get_training_args(\n",
        "            log_dir,\n",
        "            batch_eval_metrics = False,\n",
        "            max_steps = 10,\n",
        "            save_steps = 0,\n",
        "            eval_steps = 1,\n",
        "            warmup_steps = 0,\n",
        "            logging_steps = 1,\n",
        "            eval_on_start = False,\n",
        "            learning_rate = 5e-6,\n",
        "            weight_decay = 0.01,\n",
        "            )\n",
        "        else:\n",
        "            training_args = get_training_args(\n",
        "            log_dir,\n",
        "            batch_eval_metrics = False,\n",
        "            max_steps = 1000,\n",
        "            save_steps = 1005,\n",
        "            eval_steps = 100,\n",
        "            warmup_steps = 100,\n",
        "            logging_steps = 10,\n",
        "            eval_on_start = False,\n",
        "            learning_rate = 2.5e-4,\n",
        "            weight_decay = 0.01,\n",
        "            )\n",
        "\n",
        "        return training_args\n",
        "\n",
        "    param = Dimensions(\n",
        "        mels=128,\n",
        "        aud_ctx=1500,\n",
        "        aud_head=4,\n",
        "        aud_dims=512,\n",
        "        aud_idx=4,\n",
        "        vocab=40000,\n",
        "        text_ctx=512,\n",
        "        text_head=4,\n",
        "        text_dims=512,\n",
        "        text_idx=4,\n",
        "        act=\"swish\",\n",
        "        debug={\"rotary_detail\"},\n",
        "        cross_attn=True,\n",
        "        features = [\"spectrogram\"]\n",
        "        )\n",
        "\n",
        "    sanity_check = True\n",
        "\n",
        "    training_args = sanity(sanity_check)\n",
        "    dataset_config = {\n",
        "        \"spectrogram\": True,\n",
        "        \"waveforms\": False,\n",
        "        \"pitch\": False,\n",
        "        \"downsamples\": False,\n",
        "        \"frequency\": True,\n",
        "        \"hilbert\": False,\n",
        "        \"hop_length\": 128,\n",
        "        \"fmin\": 150,\n",
        "        \"fmax\": 2000,\n",
        "        \"n_mels\": 128,\n",
        "        \"n_fft\": 1024,\n",
        "        \"sampling_rate\": 16000,\n",
        "        \"pad_mode\": \"constant\",\n",
        "        \"center\": True,\n",
        "        \"power\": 2.0,\n",
        "        \"window_fn\": torch.hann_window,\n",
        "        \"mel_scale\": \"htk\",\n",
        "        \"norm\": None,\n",
        "        \"normalized\": False}\n",
        "\n",
        "    model = create_model(param)\n",
        "\n",
        "    global global_model\n",
        "    global_model = model\n",
        "\n",
        "    metrics_fn = partial(compute_metrics, print_pred=False, num_samples=5,\n",
        "                    tokenizer=tokenizer, model=model)\n",
        "\n",
        "    print(f\"{'Sanity check' if sanity_check else 'Training'} mode\")\n",
        "    train_dataset, test_dataset = prepare_datasets(\n",
        "        tokenizer=tokenizer,\n",
        "        token=token,\n",
        "        sanity_check=sanity_check,\n",
        "        dataset_config=dataset_config)\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        args=training_args,\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        data_collator=DataCollator(tokenizer=tokenizer),\n",
        "        compute_metrics=metrics_fn,\n",
        "        )\n",
        "\n",
        "    model.init_weights()\n",
        "    trainer.train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ]
}
